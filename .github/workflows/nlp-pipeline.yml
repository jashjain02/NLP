name: NLP Finance Pipeline

on:
  schedule:
    # Run daily at 6 AM UTC (after market close)
    - cron: '0 6 * * 1-5'
  workflow_dispatch:
    inputs:
      ticker:
        description: 'Stock ticker to analyze'
        required: true
        default: 'AAPL'
      lookback_days:
        description: 'Number of lookback days'
        required: true
        default: '120'
        type: string
      force_retrain:
        description: 'Force model retraining'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.9'
  PIP_CACHE_DIR: ~/.cache/pip

jobs:
  # Job 1: Data Collection and Preprocessing
  data-collection:
    runs-on: ubuntu-latest
    outputs:
      data-ready: ${{ steps.collect.outputs.data-ready }}
      ticker: ${{ steps.collect.outputs.ticker }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create data directories
      run: |
        mkdir -p data/raw data/processed models reports/figs
        
    - name: Set up environment variables
      run: |
        echo "NEWSAPI_KEY=${{ secrets.NEWSAPI_KEY }}" >> $GITHUB_ENV
        echo "MARKETAUX_API_KEY=${{ secrets.MARKETAUX_API_KEY }}" >> $GITHUB_ENV
        echo "TICKER=${{ github.event.inputs.ticker || 'AAPL' }}" >> $GITHUB_ENV
        echo "LOOKBACK_DAYS=${{ github.event.inputs.lookback_days || '120' }}" >> $GITHUB_ENV
        
    - name: Collect stock data
      id: stock-data
      run: |
        python -c "
        import yfinance as yf
        import pandas as pd
        from datetime import datetime, timedelta
        
        ticker = '${{ env.TICKER }}'
        lookback_days = int('${{ env.LOOKBACK_DAYS }}')
        
        print(f'Fetching data for {ticker}...')
        
        # Calculate date range
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days + 30)  # Extra buffer
        
        # Fetch data
        stock = yf.Ticker(ticker)
        hist = stock.history(start=start_date, end=end_date)
        
        if hist.empty:
            print(f'No data found for {ticker}')
            exit(1)
            
        # Save data
        hist.to_csv(f'data/raw/{ticker}_stock_data.csv')
        print(f'Stock data saved: {len(hist)} records')
        print('data-ready=true' >> $GITHUB_OUTPUT)
        print('ticker=' + ticker >> $GITHUB_OUTPUT)
        "
        
    - name: Collect news data
      if: steps.stock-data.outputs.data-ready == 'true'
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from data_collection.news import collect_news_multi_source
        import os
        
        ticker = '${{ env.TICKER }}'
        lookback_days = int('${{ env.LOOKBACK_DAYS }}')
        
        print(f'Collecting news for {ticker}...')
        
        try:
            # Collect news from multiple sources
            news_df = collect_news_multi_source(
                ticker=ticker,
                lookback_days=lookback_days,
                max_articles=500
            )
            
            if not news_df.empty:
                news_df.to_csv(f'data/raw/{ticker}_news_data.csv', index=False)
                print(f'News data saved: {len(news_df)} articles')
            else:
                print('No news data collected')
                
        except Exception as e:
            print(f'News collection failed: {e}')
            # Continue without news data
        "
        
    - name: Upload data artifacts
      uses: actions/upload-artifact@v3
      with:
        name: raw-data-${{ env.TICKER }}
        path: data/raw/
        retention-days: 7

  # Job 2: Model Training and Evaluation
  model-training:
    needs: data-collection
    if: needs.data-collection.outputs.data-ready == 'true'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download data artifacts
      uses: actions/download-artifact@v3
      with:
        name: raw-data-${{ needs.data-collection.outputs.ticker }}
        path: data/raw/
        
    - name: Create directories
      run: |
        mkdir -p data/processed models reports/figs
        
    - name: Set up environment variables
      run: |
        echo "NEWSAPI_KEY=${{ secrets.NEWSAPI_KEY }}" >> $GITHUB_ENV
        echo "MARKETAUX_API_KEY=${{ secrets.MARKETAUX_API_KEY }}" >> $GITHUB_ENV
        echo "TICKER=${{ needs.data-collection.outputs.ticker }}" >> $GITHUB_ENV
        
    - name: Run preprocessing
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from preprocessing.preprocess import preprocess_data
        import pandas as pd
        import os
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Preprocessing data for {ticker}...')
        
        try:
            # Load stock data
            stock_df = pd.read_csv(f'data/raw/{ticker}_stock_data.csv', index_col=0, parse_dates=True)
            
            # Load news data if available
            news_df = None
            if os.path.exists(f'data/raw/{ticker}_news_data.csv'):
                news_df = pd.read_csv(f'data/raw/{ticker}_news_data.csv')
                print(f'Loaded {len(news_df)} news articles')
            
            # Preprocess data
            processed_df = preprocess_data(stock_df, news_df, ticker)
            
            if processed_df is not None and not processed_df.empty:
                processed_df.to_csv(f'data/processed/{ticker}_processed.csv')
                print(f'Preprocessed data saved: {len(processed_df)} records')
            else:
                print('Preprocessing failed - no data produced')
                exit(1)
                
        except Exception as e:
            print(f'Preprocessing failed: {e}')
            exit(1)
        "
        
    - name: Run NLP sentiment analysis
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from nlp.sentiment import analyze_sentiment_batch
        import pandas as pd
        import os
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Running sentiment analysis for {ticker}...')
        
        try:
            # Load processed data
            df = pd.read_csv(f'data/processed/{ticker}_processed.csv')
            
            if 'text' in df.columns and not df['text'].isna().all():
                # Run sentiment analysis
                sentiment_df = analyze_sentiment_batch(df)
                
                if sentiment_df is not None:
                    sentiment_df.to_csv(f'data/processed/{ticker}_with_sentiment.csv', index=False)
                    print(f'Sentiment analysis completed: {len(sentiment_df)} records')
                else:
                    print('Sentiment analysis failed')
            else:
                print('No text data found for sentiment analysis')
                
        except Exception as e:
            print(f'Sentiment analysis failed: {e}')
        "
        
    - name: Run feature engineering
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from features.features import create_features
        import pandas as pd
        import os
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Creating features for {ticker}...')
        
        try:
            # Load data with sentiment
            sentiment_file = f'data/processed/{ticker}_with_sentiment.csv'
            if os.path.exists(sentiment_file):
                df = pd.read_csv(sentiment_file)
            else:
                df = pd.read_csv(f'data/processed/{ticker}_processed.csv')
            
            # Create features
            features_df = create_features(df, ticker)
            
            if features_df is not None and not features_df.empty:
                features_df.to_csv(f'data/processed/{ticker}_features.csv', index=False)
                print(f'Features created: {len(features_df)} records, {len(features_df.columns)} features')
            else:
                print('Feature engineering failed')
                exit(1)
                
        except Exception as e:
            print(f'Feature engineering failed: {e}')
            exit(1)
        "
        
    - name: Train models
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from modeling.modeling import train_classifiers, train_regressors
        import pandas as pd
        import os
        import json
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Training models for {ticker}...')
        
        try:
            # Load features
            df = pd.read_csv(f'data/processed/{ticker}_features.csv')
            
            # Prepare features and targets
            feature_cols = [col for col in df.columns if col not in ['date', 'target_direction', 'target_return']]
            X = df[feature_cols].fillna(0)
            
            # Classification target
            y_clf = df['target_direction'].fillna(0)
            
            # Regression target  
            y_reg = df['target_return'].fillna(0)
            
            print(f'Training data: {X.shape[0]} samples, {X.shape[1]} features')
            
            # Train classifiers
            print('Training classifiers...')
            clf_results = train_classifiers(X, y_clf, ticker)
            
            # Train regressors
            print('Training regressors...')
            reg_results = train_regressors(X, y_reg, ticker)
            
            # Save results
            results = {
                'ticker': ticker,
                'classifiers': clf_results,
                'regressors': reg_results,
                'data_shape': X.shape,
                'timestamp': pd.Timestamp.now().isoformat()
            }
            
            with open(f'reports/metrics_{ticker}.json', 'w') as f:
                json.dump(results, f, indent=2, default=str)
                
            print('Model training completed successfully')
            
        except Exception as e:
            print(f'Model training failed: {e}')
            exit(1)
        "
        
    - name: Run backtesting
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from backtest.backtest import run_backtest
        import pandas as pd
        import json
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Running backtest for {ticker}...')
        
        try:
            # Load features and models
            df = pd.read_csv(f'data/processed/{ticker}_features.csv')
            
            # Run backtest
            backtest_results = run_backtest(df, ticker)
            
            if backtest_results:
                # Save backtest results
                with open(f'reports/backtest_{ticker}.json', 'w') as f:
                    json.dump(backtest_results, f, indent=2, default=str)
                    
                print(f'Backtest completed: Sharpe={backtest_results.get(\"sharpe_ratio\", \"N/A\")}')
            else:
                print('Backtest failed')
                
        except Exception as e:
            print(f'Backtest failed: {e}')
        "
        
    - name: Generate reports
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from evaluation.evaluate import generate_diagnostic_plots
        import pandas as pd
        import os
        
        ticker = '${{ env.TICKER }}'
        
        print(f'Generating reports for {ticker}...')
        
        try:
            # Load data
            df = pd.read_csv(f'data/processed/{ticker}_features.csv')
            
            # Generate plots
            generate_diagnostic_plots(df, ticker)
            
            print('Reports generated successfully')
            
        except Exception as e:
            print(f'Report generation failed: {e}')
        "
        
    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: models-${{ needs.data-collection.outputs.ticker }}
        path: models/
        retention-days: 30
        
    - name: Upload report artifacts
      uses: actions/upload-artifact@v3
      with:
        name: reports-${{ needs.data-collection.outputs.ticker }}
        path: reports/
        retention-days: 30

  # Job 3: Deploy to Streamlit Cloud (if changes detected)
  deploy:
    needs: [data-collection, model-training]
    if: always() && (needs.data-collection.outputs.data-ready == 'true')
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Test Streamlit app
      run: |
        python -c "
        import streamlit as st
        import sys
        sys.path.append('.')
        
        # Test imports
        try:
            from ui.streamlit_app import main
            print('Streamlit app imports successful')
        except Exception as e:
            print(f'Streamlit app test failed: {e}')
            exit(1)
        "
        
    - name: Create deployment summary
      run: |
        echo "## 🚀 NLP Finance Pipeline Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Ticker Analyzed:** ${{ needs.data-collection.outputs.ticker }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ✅ Pipeline completed successfully" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Generated Artifacts:" >> $GITHUB_STEP_SUMMARY
        echo "- Raw data (stock + news)" >> $GITHUB_STEP_SUMMARY
        echo "- Processed features" >> $GITHUB_STEP_SUMMARY
        echo "- Trained models" >> $GITHUB_STEP_SUMMARY
        echo "- Performance reports" >> $GITHUB_STEP_SUMMARY
        echo "- Backtest results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🔗 Next Steps:" >> $GITHUB_STEP_SUMMARY
        echo "1. Check Streamlit Cloud for auto-deployment" >> $GITHUB_STEP_SUMMARY
        echo "2. Review generated reports in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "3. Monitor model performance" >> $GITHUB_STEP_SUMMARY

  # Job 4: Notification (optional)
  notify:
    needs: [data-collection, model-training, deploy]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
    - name: Pipeline status
      run: |
        if [ "${{ needs.data-collection.result }}" == "success" ] && [ "${{ needs.model-training.result }}" == "success" ]; then
          echo "✅ Pipeline completed successfully for ${{ needs.data-collection.outputs.ticker }}"
        else
          echo "❌ Pipeline failed - check logs for details"
        fi
